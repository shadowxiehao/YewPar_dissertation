@book{BK08,
  author    = {C. Baier and J.-P. Katoen},
  title     = {Principles of Model Checking},
  publisher = {MIT Press},
  year      = {2008},
  volume    = {},
  series    = {},
  address   = {},
  edition   = {},
  month     = {},
  note      = {},
  key       = {}
}

@inproceedings{10.1145/3332466.3374537,
  author    = {Archibald, Blair and Maier, Patrick and Stewart, Robert and Trinder, Phil},
  title     = {YewPar: Skeletons for Exact Combinatorial Search},
  year      = {2020},
  isbn      = {9781450368186},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3332466.3374537},
  doi       = {10.1145/3332466.3374537},
  abstract  = {Combinatorial search is central to many applications, yet the huge irregular search trees and the need to respect search heuristics make it hard to parallelise. We aim to improve the reuse of intricate parallel search implementations by providing the first general purpose scalable parallel framework for exact combinatorial search, YewPar.We make the following contributions. (1) We present a novel formal model of parallel backtracking search, covering enumeration, decision, and optimisation search. (2) We introduce Lazy Node Generators as a uniform API for search tree generation. (3) We present the design and implementation of 12 widely applicable algorithmic skeletons for tree search on shared and distributed memory architectures. (4) Uniquely in the field we demonstrate how a wide range of parallel search applications can easily be constructed by composing Lazy Node Generators and the search skeletons. (5) We report a systematic performance analysis of all 12 YewPar skeletons on standard instances of 7 search applications, investigating skeleton overheads and scalability up to 255 workers on 17 distributed locations.},
  booktitle = {Proceedings of the 25th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages     = {292-307},
  numpages  = {16},
  keywords  = {distributed memory parallelism, HPX, combinatorial search, algorithmic skeletons},
  location  = {San Diego, California},
  series    = {PPoPP '20}
}

@inproceedings{10.1145/2676870.2676883,
  author    = {Kaiser, Hartmut and Heller, Thomas and Adelstein-Lelbach, Bryce and Serio, Adrian and Fey, Dietmar},
  title     = {HPX: A Task Based Programming Model in a Global Address Space},
  year      = {2014},
  isbn      = {9781450332477},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2676870.2676883},
  doi       = {10.1145/2676870.2676883},
  abstract  = {The significant increase in complexity of Exascale platforms due to energy-constrained, billion-way parallelism, with major changes to processor and memory architecture, requires new energy-efficient and resilient programming techniques that are portable across multiple future generations of machines. We believe that guaranteeing adequate scalability, programmability, performance portability, resilience, and energy efficiency requires a fundamentally new approach, combined with a transition path for existing scientific applications, to fully explore the rewards of todays and tomorrows systems. We present HPX -- a parallel runtime system which extends the C++11/14 standard to facilitate distributed operations, enable fine-grained constraint based parallelism, and support runtime adaptive resource management. This provides a widely accepted API enabling programmability, composability and performance portability of user applications. By employing a global address space, we seamlessly augment the standard to apply to a distributed case. We present HPX's architecture, design decisions, and results selected from a diverse set of application runs showing superior performance, scalability, and efficiency over conventional practice.},
  booktitle = {Proceedings of the 8th International Conference on Partitioned Global Address Space Programming Models},
  articleno = {6},
  numpages  = {11},
  keywords  = {High Performance Computing, Global Address Space, Exascale, Programming Models, Parallel Runtime Systems},
  location  = {Eugene, OR, USA},
  series    = {PGAS '14}
}

@inproceedings{10.1145/1248377.1248396,
  author    = {Chen, Shimin and Gibbons, Phillip B. and Kozuch, Michael and Liaskovitis, Vasileios and Ailamaki, Anastassia and Blelloch, Guy E. and Falsafi, Babak and Fix, Limor and Hardavellas, Nikos and Mowry, Todd C. and Wilkerson, Chris},
  title     = {Scheduling Threads for Constructive Cache Sharing on CMPs},
  year      = {2007},
  isbn      = {9781595936677},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1248377.1248396},
  doi       = {10.1145/1248377.1248396},
  abstract  = {In chip multiprocessors (CMPs), limiting the number of offchip cache misses is crucial for good performance. Many multithreaded programs provide opportunities for constructive cache sharing, in which concurrently scheduled threads share a largely overlapping working set. In this paper, we compare the performance of two state-of-the-art schedulers proposed for fine-grained multithreaded programs: Parallel Depth First (PDF), which is specifically designed for constructive cache sharing, and Work Stealing (WS), which is a more traditional design. Our experimental results indicate that PDF scheduling yields a 1.3--1.6X performance improvement relative to WS for several fine-grain parallel benchmarks on projected future CMP configurations; we also report several issues that may limit the advantage of PDF in certain applications. These results also indicate that PDF more effectively utilizes off-chip bandwidth, making it possible to trade-off on-chip cache for a larger number of cores. Moreover, we find that task granularity plays a key role in cache performance. Therefore, we present an automatic approach for selecting effective grain sizes, based on a new working set profiling algorithm that is an order of magnitude faster than previous approaches. This is the first paper demonstrating the effectiveness of PDF on real benchmarks, providing a direct comparison between PDF and WS, revealing the limiting factors for PDF in practice, and presenting an approach for overcoming these factors.},
  booktitle = {Proceedings of the Nineteenth Annual ACM Symposium on Parallel Algorithms and Architectures},
  pages     = {105-115},
  numpages  = {11},
  keywords  = {thread granularity, constructive cache sharing, work stealing, parallel depth first, working set profiling, scheduling algorithms, chip multiprocessors},
  location  = {San Diego, California, USA},
  series    = {SPAA '07}
}

@article{10.1145/324133.324234,
  author     = {Blumofe, Robert D. and Leiserson, Charles E.},
  title      = {Scheduling Multithreaded Computations by Work Stealing},
  year       = {1999},
  issue_date = {Sept. 1999},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {46},
  number     = {5},
  issn       = {0004-5411},
  url        = {https://doi.org/10.1145/324133.324234},
  doi        = {10.1145/324133.324234},
  abstract   = {This paper studies the problem of efficiently schedulling fully strict (i.e., well-structured) multithreaded computations on parallel computers. A popular and practical method of scheduling this kind of dynamic MIMD-style computation is “work stealing,” in which processors needing work steal computational threads from other processors. In this paper, we give the first provably good work-stealing scheduler for multithreaded computations with dependencies.Specifically, our analysis shows that the expected time to execute a fully strict computation on P processors using our work-stealing scheduler is T1/P + O(T ∞ , where T1 is the minimum serial execution time of the multithreaded computation and (T ∞ is the minimum execution time with an infinite number of processors. Moreover, the space required by the execution is at most S1P, where S1 is the minimum serial space requirement. We also show that the expected total communication of the algorithm is at most O(PT ∞( 1 + nd)Smax), where Smax is the size of the largest activation record of any thread and nd is the maximum number of times that any thread synchronizes with its parent. This communication bound justifies the folk wisdom that work-stealing schedulers are more communication efficient than their work-sharing counterparts. All three of these bounds are existentially optimal to within a constant factor.},
  journal    = {J. ACM},
  month      = {sep},
  pages      = {720-748},
  numpages   = {29},
  keywords   = {thread scheduling, critical-path length, randomized algorithm, multithreading, multiprocessor, work stealing}
}

@inproceedings{10.1145/2851141.2851175,
  author    = {Parikh, Hrushit and Deodhar, Vinit and Gavrilovska, Ada and Pande, Santosh},
  title     = {Efficient Distributed Workstealing via Matchmaking},
  year      = {2016},
  isbn      = {9781450340922},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2851141.2851175},
  doi       = {10.1145/2851141.2851175},
  abstract  = {Many classes of high-performance applications and combinatorial problems exhibit large degree of runtime load variability. One approach to achieving balanced resource use is to over decompose the problem on fine-grained tasks that are then dynamically balanced using approaches such as workstealing. Existing work stealing techniques for such irregular applications, running on large clusters, exhibit high overheads due to potential untimely interruption of busy nodes, excessive communication messages and delays experienced by idle nodes in finding work due to repeated failed steals. We contend that the fundamental problem of distributed work-stealing is of rapidly bringing together work producers and consumers. In response, we develop an algorithm that performs timely, lightweight and highly efficient matchmaking between work producers and consumers which results in accurate load balance. Experimental evaluations show that our scheduler is able to outperform other distributed work stealing schedulers, and to achieve scale beyond what is possible with current approaches.},
  booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  articleno = {37},
  numpages  = {2},
  keywords  = {work-stealing, irregular applications, scheduling},
  location  = {Barcelona, Spain},
  series    = {PPoPP '16}
}

@inproceedings{10.1007/978-3-030-29400-7_14,
  author    = {Archibald, Blair and Maier, Patrick and Stewart, Robert and Trinder, Phil},
  title     = {Implementing YewPar: A Framework for Parallel Tree Search},
  year      = {2019},
  isbn      = {978-3-030-29399-4},
  publisher = {Springer-Verlag},
  address   = {Berlin, Heidelberg},
  url       = {https://doi.org/10.1007/978-3-030-29400-7_14},
  doi       = {10.1007/978-3-030-29400-7_14},
  abstract  = {Combinatorial search is central to many applications yet hard to parallelise. We argue for improving the reuse of parallel searches, and present the design and implementation of a new parallel search framework. YewPar generalises search by abstracting search tree generation, and by providing algorithmic skeletons that support three search types, together with a set of search coordination strategies. The evaluation shows that the cost of YewPar generality is low (6.1%); global knowledge is inexpensively shared between workers; irregular tasks are effectively distributed; and YewPar delivers good runtimes, speedups and efficiency with up to 255 workers on 17 localities.},
  booktitle = {Euro-Par 2019: Parallel Processing: 25th International Conference on Parallel and Distributed Computing, G\"{o}ttingen, Germany, August 26–30, 2019, Proceedings},
  pages     = {184-196},
  numpages  = {13},
  keywords  = {Parallel search, Exact combinatorial search, HPX},
  location  = {G\"{o}ttingen, Germany}
}

@article{GARDNER2006637,
  title    = {Exponential smoothing: The state of the art—Part II},
  journal  = {International Journal of Forecasting},
  volume   = {22},
  number   = {4},
  pages    = {637-666},
  year     = {2006},
  issn     = {0169-2070},
  doi      = {https://doi.org/10.1016/j.ijforecast.2006.03.005},
  url      = {https://www.sciencedirect.com/science/article/pii/S0169207006000392},
  author   = {Everette S. Gardner},
  keywords = {Time series—ARIMA, exponential smoothing, state-space models, identification, stability, invertibility, model selection, Comparative methods—evaluation, Intermittent demand, Inventory control, Prediction intervals, Regression—discount weighted, kernel},
  abstract = {In Gardner [Gardner, E. S., Jr. (1985). Exponential smoothing: The state of the art. Journal of Forecasting 4, 1–28], I reviewed the research in exponential smoothing since the original work by Brown and Holt. This paper brings the state of the art up to date. The most important theoretical advance is the invention of a complete statistical rationale for exponential smoothing based on a new class of state-space models with a single source of error. The most important practical advance is the development of a robust method for smoothing damped multiplicative trends. We also have a new adaptive method for simple smoothing, the first such method to demonstrate credible improved forecast accuracy over fixed-parameter smoothing. Longstanding confusion in the literature about whether and how to renormalize seasonal indices in the Holt–Winters methods has finally been resolved. There has been significant work in forecasting for inventory control, including the development of new predictive distributions for total lead-time demand and several improved versions of Croston's method for forecasting intermittent time series. Regrettably, there has been little progress in the identification and selection of exponential smoothing methods. The research in this area is best described as inconclusive, and it is still difficult to beat the application of a damped trend to every time series.}
}

@article{fromentin2016exploring,
  author   = {Jean Fromentin and Florent Hivert},
  title    = {Exploring the tree of numerical semigroups},
  journal  = {Math. Comp.},
  volume   = {85},
  number   = {301},
  pages    = {2553--2568},
  year     = {2016},
  url      = {https://doi.org/10.1090/mcom/3075},
  abstract = {In this paper we describe an algorithm visiting all numerical semigroups up to a given genus using a well-suited representation. The interest of this algorithm is that it fits particularly well the architecture of modern computers allowing very large optimizations: we obtain the number of numerical semigroups of genus and we confirm the Wilf conjecture for.}
}

@inproceedings{olivier2006uts,
  title     = {UTS: An unbalanced tree search benchmark},
  author    = {Olivier, Stephen and Huan, Jun and Liu, Jinze and Prins, Jan and Dinan, James and Sadayappan, P and Tseng, Chau-Wen},
  booktitle = {International Workshop on Languages and Compilers for Parallel Computing},
  pages     = {235--250},
  year      = {2006},
  publisher = {Springer}
}

@inproceedings{10.1007/3-540-60321-2_29,
  author    = {de Bruin, A.
               and Kindervater, G. A. P.
               and Trienekens, H. W. J. M.},
  editor    = {Ferreira, Afonso
               and Rolim, Jos{\'e}},
  title     = {Asynchronous parallel branch and bound and anomalies},
  booktitle = {Parallel Algorithms for Irregularly Structured Problems},
  year      = {1995},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {363--377},
  abstract  = {The parallel execution of branch and bound algorithms can result in seemingly unreasonable speedups or slowdowns. Almost never the speedup is equal to the increase in computing power. For synchronous parallel branch and bound, these effects have been studied extensively. For asynchronous parallelizations, only little is known.},
  isbn      = {978-3-540-44915-7}
}

@article{ARCHIBALD201892,
  title    = {Replicable parallel branch and bound search},
  journal  = {Journal of Parallel and Distributed Computing},
  volume   = {113},
  pages    = {92-114},
  year     = {2018},
  issn     = {0743-7315},
  doi      = {https://doi.org/10.1016/j.jpdc.2017.10.010},
  url      = {https://www.sciencedirect.com/science/article/pii/S0743731517302861},
  author   = {Blair Archibald and Patrick Maier and Ciaran McCreesh and Robert Stewart and Phil Trinder},
  keywords = {Algorithmic skeletons, Branch-and-bound, Parallel algorithms, Combinatorial optimisation, Distributed computing, Repeatability},
  abstract = {Combinatorial branch and bound searches are a common technique for solving global optimisation and decision problems. Their performance often depends on good search order heuristics, refined over decades of algorithms research. Parallel search necessarily deviates from the sequential search order, sometimes dramatically and unpredictably, e.g. by distributing work at random. This can disrupt effective search order heuristics and lead to unexpected and highly variable parallel performance. The variability makes it hard to reason about the parallel performance of combinatorial searches. This paper presents a generic parallel branch and bound skeleton, implemented in Haskell, with replicable parallel performance. The skeleton aims to preserve the search order heuristic by distributing work in an ordered fashion, closely following the sequential search order. We demonstrate the generality of the approach by applying the skeleton to 40 instances of three combinatorial problems: Maximum Clique, 0/1 Knapsack and Travelling Salesperson. The overheads of our Haskell skeleton are reasonable: giving slowdown factors of between 1.9 and 6.2 compared with a class-leading, dedicated, and highly optimised C++ Maximum Clique solver. We demonstrate scaling up to 200 cores of a Beowulf cluster, achieving speedups of 100x for several Maximum Clique instances. We demonstrate low variance of parallel performance across all instances of the three combinatorial problems and at all scales up to 200 cores, with median Relative Standard Deviation (RSD) below 2%. Parallel solvers that do not follow the sequential search order exhibit far higher variance, with median RSD exceeding 85% for Knapsack.}
}

@article{mccreesh2013multithreading,
  author  = {Ciaran McCreesh and Patrick Prosser},
  title   = {Multi-threading a state-of-the-art maximum clique algorithm},
  journal = {Algorithms},
  volume  = {6},
  number  = {4},
  year    = {2013},
  pages   = {618--635}
}

@phdthesis{archibald2018,
  author = {Blair Archibald},
  title  = {Skeletons for Exact Combinatorial Search at Scale},
  year   = {2018},
  school = {University of Glasgow},
  url    = {http://theses.gla.ac.uk/id/eprint/31000}
}

@article{archibald2019yewpar,
  author = {Blair Archibald and Patrick Maier and Phil Trinder and Robert Stewart},
  title  = {YewPar: Skeletons for Exact Combinatorial Search [Data Collection]},
  year   = {2019},
  url    = {https://doi.org/10.5525/gla.researchdata.935},
  note   = {(2019)}
}