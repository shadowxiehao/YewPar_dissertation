
@book{BK08,
  author    = {C. Baier and J.-P. Katoen},
  title     = {Principles of Model Checking},
  publisher = {MIT Press},
  year      = {2008},
  volume    = {},
  series    = {},
  address   = {},
  edition   = {},
  month     = {},
  note      = {},
  key       = {}
}

@inproceedings{10.1145/3332466.3374537,
  author    = {Archibald, Blair and Maier, Patrick and Stewart, Robert and Trinder, Phil},
  title     = {YewPar: Skeletons for Exact Combinatorial Search},
  year      = {2020},
  isbn      = {9781450368186},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3332466.3374537},
  doi       = {10.1145/3332466.3374537},
  abstract  = {Combinatorial search is central to many applications, yet the huge irregular search trees and the need to respect search heuristics make it hard to parallelise. We aim to improve the reuse of intricate parallel search implementations by providing the first general purpose scalable parallel framework for exact combinatorial search, YewPar.We make the following contributions. (1) We present a novel formal model of parallel backtracking search, covering enumeration, decision, and optimisation search. (2) We introduce Lazy Node Generators as a uniform API for search tree generation. (3) We present the design and implementation of 12 widely applicable algorithmic skeletons for tree search on shared and distributed memory architectures. (4) Uniquely in the field we demonstrate how a wide range of parallel search applications can easily be constructed by composing Lazy Node Generators and the search skeletons. (5) We report a systematic performance analysis of all 12 YewPar skeletons on standard instances of 7 search applications, investigating skeleton overheads and scalability up to 255 workers on 17 distributed locations.},
  booktitle = {Proceedings of the 25th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages     = {292–307},
  numpages  = {16},
  keywords  = {distributed memory parallelism, HPX, combinatorial search, algorithmic skeletons},
  location  = {San Diego, California},
  series    = {PPoPP '20}
}

@inproceedings{10.1145/2676870.2676883,
  author    = {Kaiser, Hartmut and Heller, Thomas and Adelstein-Lelbach, Bryce and Serio, Adrian and Fey, Dietmar},
  title     = {HPX: A Task Based Programming Model in a Global Address Space},
  year      = {2014},
  isbn      = {9781450332477},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2676870.2676883},
  doi       = {10.1145/2676870.2676883},
  abstract  = {The significant increase in complexity of Exascale platforms due to energy-constrained, billion-way parallelism, with major changes to processor and memory architecture, requires new energy-efficient and resilient programming techniques that are portable across multiple future generations of machines. We believe that guaranteeing adequate scalability, programmability, performance portability, resilience, and energy efficiency requires a fundamentally new approach, combined with a transition path for existing scientific applications, to fully explore the rewards of todays and tomorrows systems. We present HPX -- a parallel runtime system which extends the C++11/14 standard to facilitate distributed operations, enable fine-grained constraint based parallelism, and support runtime adaptive resource management. This provides a widely accepted API enabling programmability, composability and performance portability of user applications. By employing a global address space, we seamlessly augment the standard to apply to a distributed case. We present HPX's architecture, design decisions, and results selected from a diverse set of application runs showing superior performance, scalability, and efficiency over conventional practice.},
  booktitle = {Proceedings of the 8th International Conference on Partitioned Global Address Space Programming Models},
  articleno = {6},
  numpages  = {11},
  keywords  = {High Performance Computing, Global Address Space, Exascale, Programming Models, Parallel Runtime Systems},
  location  = {Eugene, OR, USA},
  series    = {PGAS '14}
}

@inproceedings{10.1145/1248377.1248396,
  author    = {Chen, Shimin and Gibbons, Phillip B. and Kozuch, Michael and Liaskovitis, Vasileios and Ailamaki, Anastassia and Blelloch, Guy E. and Falsafi, Babak and Fix, Limor and Hardavellas, Nikos and Mowry, Todd C. and Wilkerson, Chris},
  title     = {Scheduling Threads for Constructive Cache Sharing on CMPs},
  year      = {2007},
  isbn      = {9781595936677},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1248377.1248396},
  doi       = {10.1145/1248377.1248396},
  abstract  = {In chip multiprocessors (CMPs), limiting the number of offchip cache misses is crucial for good performance. Many multithreaded programs provide opportunities for constructive cache sharing, in which concurrently scheduled threads share a largely overlapping working set. In this paper, we compare the performance of two state-of-the-art schedulers proposed for fine-grained multithreaded programs: Parallel Depth First (PDF), which is specifically designed for constructive cache sharing, and Work Stealing (WS), which is a more traditional design. Our experimental results indicate that PDF scheduling yields a 1.3--1.6X performance improvement relative to WS for several fine-grain parallel benchmarks on projected future CMP configurations; we also report several issues that may limit the advantage of PDF in certain applications. These results also indicate that PDF more effectively utilizes off-chip bandwidth, making it possible to trade-off on-chip cache for a larger number of cores. Moreover, we find that task granularity plays a key role in cache performance. Therefore, we present an automatic approach for selecting effective grain sizes, based on a new working set profiling algorithm that is an order of magnitude faster than previous approaches. This is the first paper demonstrating the effectiveness of PDF on real benchmarks, providing a direct comparison between PDF and WS, revealing the limiting factors for PDF in practice, and presenting an approach for overcoming these factors.},
  booktitle = {Proceedings of the Nineteenth Annual ACM Symposium on Parallel Algorithms and Architectures},
  pages     = {105–115},
  numpages  = {11},
  keywords  = {thread granularity, constructive cache sharing, work stealing, parallel depth first, working set profiling, scheduling algorithms, chip multiprocessors},
  location  = {San Diego, California, USA},
  series    = {SPAA '07}
}

@article{10.1145/324133.324234,
  author     = {Blumofe, Robert D. and Leiserson, Charles E.},
  title      = {Scheduling Multithreaded Computations by Work Stealing},
  year       = {1999},
  issue_date = {Sept. 1999},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {46},
  number     = {5},
  issn       = {0004-5411},
  url        = {https://doi.org/10.1145/324133.324234},
  doi        = {10.1145/324133.324234},
  abstract   = {This paper studies the problem of efficiently schedulling fully strict (i.e., well-structured) multithreaded computations on parallel computers. A popular and practical method of scheduling this kind of dynamic MIMD-style computation is “work stealing,” in which processors needing work steal computational threads from other processors. In this paper, we give the first provably good work-stealing scheduler for multithreaded computations with dependencies.Specifically, our analysis shows that the expected time to execute a fully strict computation on P processors using our work-stealing scheduler is T1/P + O(T ∞ , where T1 is the minimum serial execution time of the multithreaded computation and (T ∞ is the minimum execution time with an infinite number of processors. Moreover, the space required by the execution is at most S1P, where S1 is the minimum serial space requirement. We also show that the expected total communication of the algorithm is at most O(PT ∞( 1 + nd)Smax), where Smax is the size of the largest activation record of any thread and nd is the maximum number of times that any thread synchronizes with its parent. This communication bound justifies the folk wisdom that work-stealing schedulers are more communication efficient than their work-sharing counterparts. All three of these bounds are existentially optimal to within a constant factor.},
  journal    = {J. ACM},
  month      = {sep},
  pages      = {720–748},
  numpages   = {29},
  keywords   = {thread scheduling, critical-path length, randomized algorithm, multithreading, multiprocessor, work stealing}
}

@inproceedings{10.1145/2851141.2851175,
  author    = {Parikh, Hrushit and Deodhar, Vinit and Gavrilovska, Ada and Pande, Santosh},
  title     = {Efficient Distributed Workstealing via Matchmaking},
  year      = {2016},
  isbn      = {9781450340922},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2851141.2851175},
  doi       = {10.1145/2851141.2851175},
  abstract  = {Many classes of high-performance applications and combinatorial problems exhibit large degree of runtime load variability. One approach to achieving balanced resource use is to over decompose the problem on fine-grained tasks that are then dynamically balanced using approaches such as workstealing. Existing work stealing techniques for such irregular applications, running on large clusters, exhibit high overheads due to potential untimely interruption of busy nodes, excessive communication messages and delays experienced by idle nodes in finding work due to repeated failed steals. We contend that the fundamental problem of distributed work-stealing is of rapidly bringing together work producers and consumers. In response, we develop an algorithm that performs timely, lightweight and highly efficient matchmaking between work producers and consumers which results in accurate load balance. Experimental evaluations show that our scheduler is able to outperform other distributed work stealing schedulers, and to achieve scale beyond what is possible with current approaches.},
  booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  articleno = {37},
  numpages  = {2},
  keywords  = {work-stealing, irregular applications, scheduling},
  location  = {Barcelona, Spain},
  series    = {PPoPP '16}
}


@article{10.1145/3016078.2851175,
  author     = {Parikh, Hrushit and Deodhar, Vinit and Gavrilovska, Ada and Pande, Santosh},
  title      = {Efficient Distributed Workstealing via Matchmaking},
  year       = {2016},
  issue_date = {August 2016},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {51},
  number     = {8},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/3016078.2851175},
  doi        = {10.1145/3016078.2851175},
  abstract   = {Many classes of high-performance applications and combinatorial problems exhibit large degree of runtime load variability. One approach to achieving balanced resource use is to over decompose the problem on fine-grained tasks that are then dynamically balanced using approaches such as workstealing. Existing work stealing techniques for such irregular applications, running on large clusters, exhibit high overheads due to potential untimely interruption of busy nodes, excessive communication messages and delays experienced by idle nodes in finding work due to repeated failed steals. We contend that the fundamental problem of distributed work-stealing is of rapidly bringing together work producers and consumers. In response, we develop an algorithm that performs timely, lightweight and highly efficient matchmaking between work producers and consumers which results in accurate load balance. Experimental evaluations show that our scheduler is able to outperform other distributed work stealing schedulers, and to achieve scale beyond what is possible with current approaches.},
  journal    = {SIGPLAN Not.},
  month      = {feb},
  articleno  = {37},
  numpages   = {2},
  keywords   = {irregular applications, scheduling, work-stealing}
}

@inproceedings{10.1007/978-3-030-29400-7_14,
  author    = {Archibald, Blair and Maier, Patrick and Stewart, Robert and Trinder, Phil},
  title     = {Implementing YewPar: A Framework for Parallel Tree Search},
  year      = {2019},
  isbn      = {978-3-030-29399-4},
  publisher = {Springer-Verlag},
  address   = {Berlin, Heidelberg},
  url       = {https://doi.org/10.1007/978-3-030-29400-7_14},
  doi       = {10.1007/978-3-030-29400-7_14},
  abstract  = {Combinatorial search is central to many applications yet hard to parallelise. We argue for improving the reuse of parallel searches, and present the design and implementation of a new parallel search framework. YewPar generalises search by abstracting search tree generation, and by providing algorithmic skeletons that support three search types, together with a set of search coordination strategies. The evaluation shows that the cost of YewPar generality is low (6.1%); global knowledge is inexpensively shared between workers; irregular tasks are effectively distributed; and YewPar delivers good runtimes, speedups and efficiency with up to 255 workers on 17 localities.},
  booktitle = {Euro-Par 2019: Parallel Processing: 25th International Conference on Parallel and Distributed Computing, G\"{o}ttingen, Germany, August 26–30, 2019, Proceedings},
  pages     = {184-196},
  numpages  = {13},
  keywords  = {Parallel search, Exact combinatorial search, HPX},
  location  = {G\"{o}ttingen, Germany}
}

@article{GARDNER2006637,
  title    = {Exponential smoothing: The state of the art—Part II},
  journal  = {International Journal of Forecasting},
  volume   = {22},
  number   = {4},
  pages    = {637-666},
  year     = {2006},
  issn     = {0169-2070},
  doi      = {https://doi.org/10.1016/j.ijforecast.2006.03.005},
  url      = {https://www.sciencedirect.com/science/article/pii/S0169207006000392},
  author   = {Everette S. Gardner},
  keywords = {Time series—ARIMA, exponential smoothing, state-space models, identification, stability, invertibility, model selection, Comparative methods—evaluation, Intermittent demand, Inventory control, Prediction intervals, Regression—discount weighted, kernel},
  abstract = {In Gardner [Gardner, E. S., Jr. (1985). Exponential smoothing: The state of the art. Journal of Forecasting 4, 1–28], I reviewed the research in exponential smoothing since the original work by Brown and Holt. This paper brings the state of the art up to date. The most important theoretical advance is the invention of a complete statistical rationale for exponential smoothing based on a new class of state-space models with a single source of error. The most important practical advance is the development of a robust method for smoothing damped multiplicative trends. We also have a new adaptive method for simple smoothing, the first such method to demonstrate credible improved forecast accuracy over fixed-parameter smoothing. Longstanding confusion in the literature about whether and how to renormalize seasonal indices in the Holt–Winters methods has finally been resolved. There has been significant work in forecasting for inventory control, including the development of new predictive distributions for total lead-time demand and several improved versions of Croston's method for forecasting intermittent time series. Regrettably, there has been little progress in the identification and selection of exponential smoothing methods. The research in this area is best described as inconclusive, and it is still difficult to beat the application of a damped trend to every time series.}
}